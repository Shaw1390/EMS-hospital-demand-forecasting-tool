# -*- coding: utf-8 -*-
"""Streamlit Dashboard

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12k4ZPwWEOOU0E2Zvc-Noun63mjyT9Fa7
"""

!pip install streamlit
!pip install plotly

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score


st.set_page_config(
    page_title="Toronto Climate‚ÄìHealth Dashboard",
    page_icon="‚ôïI‚Ñ¢AF",
    layout="wide"
)

st.title("Toronto Climate, Air Quality & EMS Health Dashboard")
st.markdown("""
A professional exploratory dashboard showing how **climate**,
**air pollution**,
and **digital health search trends** relate to **EMS call volume in Toronto**.
""")



@st.cache_data
def load_data():
    df = pd.read_csv("toronto_final_merged_dataset.csv")
    df["date"] = pd.to_datetime(df["date"])
    return df

df = load_data()


@st.cache_data
def preprocess_data(df_raw):
    df_processed = df_raw.copy()
    df_processed = df_processed.sort_values(by='date').reset_index(drop=True)
    df_processed = df_processed.dropna(axis=1, how='all')

    lag_cols = ['pm25', 'no2', 'so2', 'co', 'avg_temperature', 'breathing problems', 'ambulance']
    lags = range(1, 8)
    ma_windows = [3, 7]

    for col in lag_cols:
        for lag in lags:
            df_processed[f'{col}_lag{lag}'] = df_processed[col].shift(lag)
        for w in ma_windows:
            df_processed[f'{col}_ma{w}'] = df_processed[col].rolling(w).mean()

    df_processed = df_processed.dropna().reset_index(drop=True)
    return df_processed, lag_cols

df_processed, lag_cols = preprocess_data(df)


@st.cache_resource
def get_trained_model(df_model_data, lag_cols):
    feature_cols = [c for c in df_model_data.columns if any(x in c for x in lag_cols) and df_model_data[c].dtype != 'object' and c != 'date']
    X = df_model_data[feature_cols]
    y = df_model_data['ems_calls']

    split_date = '2024-01-01'
    X_train = X[df_model_data['date'] < split_date]
    y_train = y[df_model_data['date'] < split_date]
    X_test  = X[df_model_data['date'] >= split_date]
    y_test  = y[df_model_data['date'] >= split_date]

    model = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=10)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2   = r2_score(y_test, y_pred)

    importance = pd.DataFrame({
        "feature": feature_cols,
        "importance": model.feature_importances_
    }).sort_values("importance", ascending=False)

    return model, importance, X, y, y_test, y_pred, rmse, r2, feature_cols

model, importance, X_all, y_all, y_test, y_pred, rmse, r2, feature_cols = get_trained_model(df_processed, lag_cols)

st.sidebar.header("üîç Filters")

start_date = st.sidebar.date_input("Start Date", df_processed["date"].min().date())
end_date   = st.sidebar.date_input("End Date", df_processed["date"].max().date())

mask = (df_processed["date"] >= pd.to_datetime(start_date)) & \
       (df_processed["date"] <= pd.to_datetime(end_date))

df_filtered = df_processed[mask]


st.subheader("üìä Summary Metrics")
col1, col2, col3 = st.columns(3)

col1.metric("Date Range", f"{start_date} ‚Üí {end_date}")
col2.metric("Total EMS Calls", int(df_filtered["ems_calls"].sum()))
col3.metric("Avg PM2.5", round(df_filtered["pm25"].mean(), 2))

st.header("üìà EMS Calls & Environmental Trends Over Time")

fig = px.line(
    df_filtered,
    x="date",
    y=["ems_calls", "pm25", "no2", "avg_temperature"],
    labels={"value": "Value", "date": "Date"},
    title="EMS Calls vs PM2.5, NO‚ÇÇ, Temperature"
)
st.plotly_chart(fig, use_container_width=True)



st.header("üî• Correlation Heatmap (Climate + Pollution + Trends + EMS)")

numeric_df = df_filtered.select_dtypes(include=np.number)

fig_corr, ax = plt.subplots(figsize=(15, 10))
sns.heatmap(numeric_df.corr(), cmap="coolwarm", center=0, ax=ax)
st.pyplot(fig_corr)


st.header("üå≤ Random Forest Feature Importance")

st.write(f"**Model RMSE:** {rmse:.2f}")
st.write(f"**Model R¬≤:** {r2:.3f}")

fig_imp = px.bar(
    importance.head(15),
    x="importance",
    y="feature",
    orientation="h",
    title="Top 15 Most Important Predictors of EMS Calls"
)
st.plotly_chart(fig_imp, use_container_width=True)


st.header("üìâ Actual vs Predicted EMS Calls")

df_pred_plot = pd.DataFrame({
    'date': df_processed['date'][df_processed['date'] >= '2024-01-01'],
    'Actual EMS Calls': y_test,
    'Predicted EMS Calls': y_pred
}).set_index('date').resample('D').mean().dropna().reset_index()

fig_pred = px.line(
    df_pred_plot,
    x="date",
    y=["Actual EMS Calls", "Predicted EMS Calls"],
    labels={"value": "EMS Calls"},
    title="Actual vs Predicted EMS Calls (2024‚Äì2025)"
)
st.plotly_chart(fig_pred, use_container_width=True)


st.header(" ‚òÅÔ∏è Forecast Future EMS Calls")
st.write("Select a future date to forecast EMS call volume.")

last_date_in_data = df_processed["date"].max().date()
forecast_date = st.date_input(
    "Select Forecast Date",
    value=last_date_in_data + pd.Timedelta(days=7),
    min_value=last_date_in_data + pd.Timedelta(days=1)
)

if st.button("Generate Forecast"):
    if pd.to_datetime(forecast_date) <= df_processed["date"].max():
        st.warning("Please select a date in the future.")
    else:

        last_row = df_processed.iloc[-1].copy()
        forecast_input = pd.DataFrame([last_row])
        forecast_input['date'] = pd.to_datetime(forecast_date)


        forecast_X = forecast_input[feature_cols]


        predicted_ems_calls = model.predict(forecast_X)[0]

        st.success(f"Forecasted EMS Calls for {forecast_date}: **{predicted_ems_calls:.2f}**")
        st.info("Note: This forecast uses a simplified extrapolation of features. For a more accurate prediction, future climate, pollution, and trend data would be required.")

st.success("Dashboard Loaded Successfully!")

# Save the Streamlit app code to a file
with open('streamlit_app.py', 'w') as f:
    f.write('''
# ===========================================
# TORONTO HEALTH‚ÄìCLIMATE DASHBOARD (Streamlit)
# ===========================================

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score


# -----------------------------
# PAGE CONFIG
# -----------------------------
st.set_page_config(
    page_title="Toronto Climate‚ÄìHealth Dashboard",
    page_icon="‚ôïI‚Ñ¢AF",
    layout="wide"
)

st.title(" ‚ôïI‚Ñ¢AF Toronto Climate, Air Quality & EMS Health Dashboard")
st.markdown("""
A professional exploratory dashboard showing how **climate**,
**air pollution**,
and **digital health search trends** relate to **EMS call volume in Toronto**.
""")


# -----------------------------
# LOAD FINAL MERGED DATA
# -----------------------------
@st.cache_data
def load_data():
    df = pd.read_csv("toronto_final_merged_dataset.csv")
    df["date"] = pd.to_datetime(df["date"])
    return df

df = load_data()

# -----------------------------
# PREPROCESS DATA FOR MODELING (Identical to training notebook)
# -----------------------------
@st.cache_data
def preprocess_data(df_raw):
    df_processed = df_raw.copy()
    df_processed = df_processed.sort_values(by='date').reset_index(drop=True)
    df_processed = df_processed.dropna(axis=1, how='all')

    lag_cols = ['pm25', 'no2', 'so2', 'co', 'avg_temperature', 'breathing problems', 'ambulance']
    lags = range(1, 8)
    ma_windows = [3, 7]

    for col in lag_cols:
        for lag in lags:
            df_processed[f'{col}_lag{lag}'] = df_processed[col].shift(lag)
        for w in ma_windows:
            df_processed[f'{col}_ma{w}'] = df_processed[col].rolling(w).mean()

    df_processed = df_processed.dropna().reset_index(drop=True)
    return df_processed, lag_cols

df_processed, lag_cols = preprocess_data(df)

# -----------------------------
# TRAIN MODEL (Cached for performance)
# -----------------------------
@st.cache_resource
def get_trained_model(df_model_data, lag_cols):
    feature_cols = [c for c in df_model_data.columns if any(x in c for x in lag_cols) and df_model_data[c].dtype != 'object' and c != 'date']
    X = df_model_data[feature_cols]
    y = df_model_data['ems_calls']

    split_date = '2024-01-01'
    X_train = X[df_model_data['date'] < split_date]
    y_train = y[df_model_data['date'] < split_date]
    X_test  = X[df_model_data['date'] >= split_date]
    y_test  = y[df_model_data['date'] >= split_date]

    model = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=10)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2   = r2_score(y_test, y_pred)

    importance = pd.DataFrame({
        "feature": feature_cols,
        "importance": model.feature_importances_
    }).sort_values("importance", ascending=False)

    return model, importance, X, y, y_test, y_pred, rmse, r2, feature_cols

model, importance, X_all, y_all, y_test, y_pred, rmse, r2, feature_cols = get_trained_model(df_processed, lag_cols)


# -----------------------------
# SIDEBAR FILTERS
# -----------------------------
st.sidebar.header("üîç Filters")

start_date = st.sidebar.date_input("Start Date", df_processed["date"].min().date())
end_date   = st.sidebar.date_input("End Date", df_processed["date"].max().date())

mask = (df_processed["date"] >= pd.to_datetime(start_date)) & \
       (df_processed["date"] <= pd.to_datetime(end_date))

df_filtered = df_processed[mask]


# -----------------------------
# SHOW SUMMARY METRICS
# -----------------------------
st.subheader("üìä Summary Metrics")
col1, col2, col3 = st.columns(3)

col1.metric("Date Range", f"{start_date} ‚Üí {end_date}")
col2.metric("Total EMS Calls", int(df_filtered["ems_calls"].sum()))
col3.metric("Avg PM2.5", round(df_filtered["pm25"].mean(), 2))


# -----------------------------
# TIME SERIES: EMS + Pollution
# -----------------------------
st.header("üìà EMS Calls & Environmental Trends Over Time")

fig = px.line(
    df_filtered,
    x="date",
    y=["ems_calls", "pm25", "no2", "avg_temperature"],
    labels={"value": "Value", "date": "Date"},
    title="EMS Calls vs PM2.5, NO‚ÇÇ, Temperature"
)
st.plotly_chart(fig, use_container_width=True)


# -----------------------------
# CORRELATION HEATMAP
# -----------------------------
st.header("üî• Correlation Heatmap (Climate + Pollution + Trends + EMS)")

numeric_df = df_filtered.select_dtypes(include=np.number)

fig_corr, ax = plt.subplots(figsize=(15, 10))
sns.heatmap(numeric_df.corr(), cmap="coolwarm", center=0, ax=ax)
st.pyplot(fig_corr)


# -----------------------------
# FEATURE IMPORTANCE (from RF)
# -----------------------------
st.header("üå≤ Random Forest Feature Importance")

st.write(f"**Model RMSE:** {rmse:.2f}")
st.write(f"**Model R¬≤:** {r2:.3f}")

fig_imp = px.bar(
    importance.head(15),
    x="importance",
    y="feature",
    orientation="h",
    title="Top 15 Most Important Predictors of EMS Calls"
)
st.plotly_chart(fig_imp, use_container_width=True)


# -----------------------------
# ACTUAL VS PREDICTED PLOT
# -----------------------------
st.header("üìâ Actual vs Predicted EMS Calls")

df_pred_plot = pd.DataFrame({
    'date': df_processed['date'][df_processed['date'] >= '2024-01-01'],
    'Actual EMS Calls': y_test,
    'Predicted EMS Calls': y_pred
}).set_index('date').resample('D').mean().dropna().reset_index()

fig_pred = px.line(
    df_pred_plot,
    x="date",
    y=["Actual EMS Calls", "Predicted EMS Calls"],
    labels={"value": "EMS Calls"},
    title="Actual vs Predicted EMS Calls (2024‚Äì2025)"
)
st.plotly_chart(fig_pred, use_container_width=True)


# -----------------------------
# FORECASTING SECTION
# -----------------------------
st.header(" ‚òÅÔ∏è Forecast Future EMS Calls")
st.write("Select a future date to forecast EMS call volume.")

last_date_in_data = df_processed["date"].max().date()
forecast_date = st.date_input(
    "Select Forecast Date",
    value=last_date_in_data + pd.Timedelta(days=7),
    min_value=last_date_in_data + pd.Timedelta(days=1)
)

if st.button("Generate Forecast"):
    if pd.to_datetime(forecast_date) <= df_processed["date"].max():
        st.warning("Please select a date in the future.")
    else:
        # To forecast, we need to generate features for the forecast date.
        # This is a simplified approach, a robust forecast would need future
        # values for climate, pollution, and trends.

        # For demonstration, we'll use the last available data point to extrapolate
        # new features. In a real scenario, you'd need external forecasts for these.
        last_row = df_processed.iloc[-1].copy()
        forecast_input = pd.DataFrame([last_row])
        forecast_input['date'] = pd.to_datetime(forecast_date)

        # Re-calculate lag features for the forecast_input DataFrame
        # This requires careful handling as simple shift won't work across new dates
        # For simplicity in this demo, we assume the last known lags and MAs persist
        # or can be intelligently imputed/forecasted themselves.
        # A more robust solution would involve a separate forecasting pipeline for each feature.

        # Filter forecast_input to only include feature_cols, ensuring order matches training
        forecast_X = forecast_input[feature_cols]

        # Predict
        predicted_ems_calls = model.predict(forecast_X)[0]

        st.success(f"Forecasted EMS Calls for {forecast_date}: **{predicted_ems_calls:.2f}**")
        st.info("Note: This forecast uses a simplified extrapolation of features. For a more accurate prediction, future climate, pollution, and trend data would be required.")

st.success("Dashboard Loaded Successfully!")
'''
)

# Run Streamlit app with ngrok
from pyngrok import ngrok
import subprocess
import os
import time

# Kill any processes running on port 8501 (Streamlit's default port)
# This is important to ensure ngrok can bind to the port
!kill $(lsof -t -i:8501)

# Start Streamlit in the background
print("Starting Streamlit app...")
streamlit_process = subprocess.Popen(
    ["streamlit", "run", "streamlit_app.py", "--server.port", "8501", "--server.enableCORS", "false", "--server.enableXsrfProtection", "false"],
    stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid
)

# Give Streamlit a few seconds to start
time.sleep(10)

# Open a ngrok tunnel to the Streamlit app's port (8501)
print("Opening ngrok tunnel...")
public_url = ngrok.connect(8501)

print("Your Streamlit dashboard is available at:")
print(public_url)
print("Click the link above to open your dashboard in a new tab.")
print("To stop the Streamlit app and ngrok tunnel, interrupt this cell's execution.")